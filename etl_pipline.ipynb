{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf6e5bd3-9c20-4b88-a55d-cacd0633929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "import pymysql\n",
    "from dotenv import load_dotenv  # For securely managing credentials\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71054c4e-1f44-4d4f-8e1a-82f7dd089e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION: Load credentials from a .env file ---\n",
    "# Create a file named \".env\" in the same directory with:\n",
    "# DB_USER=\"your_user\"\n",
    "# DB_PASS=\"your_password\"\n",
    "# DB_HOST=\"localhost\"\n",
    "# DB_NAME=\"HealthcareADT_DW\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASS = os.getenv('DB_PASS')\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "\n",
    "if not all([DB_USER, DB_PASS, DB_HOST, DB_NAME]):\n",
    "    print(\"Error: Database credentials are not set in .env file.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b73490de-c4fc-4b9a-8f1f-d47fea1cd921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding password\n",
    "import urllib.parse\n",
    "\n",
    "encoded_password = urllib.parse.quote(DB_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6a57ae4-9029-4f3e-8b01-0fe3bdbd6c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection engine created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a database connection engine\n",
    "try:\n",
    "    connection_string = f\"mysql+pymysql://{DB_USER}:{encoded_password}@{DB_HOST}/{DB_NAME}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    print(\"Database connection engine created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating database engine: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fabb3611-6945-4167-879e-0a9899ab0c6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DimDoctors',\n",
       " 'DimHospitals',\n",
       " 'DimInsurance',\n",
       " 'DimPatients',\n",
       " 'FactAdmissions',\n",
       " 'Staging_Admissions']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect(engine).get_table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de1c44a-b396-4616-871b-70129bd19cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the User Defined Function responsible for extracting data from the main csv file.\n",
    "def extract_and_validate_csv(file_path):\n",
    "    \"\"\"\n",
    "    Extracts data from CSV, validates, cleans, transforms, and\n",
    "    creates a unique business key.\n",
    "    \"\"\"\n",
    "    print(f\"Starting extraction and validation for: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # #######################\n",
    "    # --- Amending the original dataset column names to fit the script names\n",
    "    df.columns = [col.replace(\" \", \"_\") for col in df.columns]\n",
    "    # #######################\n",
    "    \n",
    "    # --- 1. Data Cleaning ---\n",
    "    # Trim all string columns\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = df[col].str.strip()\n",
    "\n",
    "    # Standardize Name to Title Case\n",
    "    df['Name'] = df['Name'].str.title()\n",
    "\n",
    "    # Convert numeric and date columns, handling errors\n",
    "    # This code reads the billing amount values evenif they have $ and , symbols\n",
    "    # It can modified to detect any other currency symbols as needed\n",
    "    df['Billing_Amount'] = pd.to_numeric(\n",
    "        df['Billing_Amount'].astype(str).str.replace(r'[$,]', '', regex=True),\n",
    "        errors='coerce'  # Bad values become NaN (NULL)\n",
    "    )\n",
    "\n",
    "    # Convert Age to numeric\n",
    "    df['Age'] = pd.to_numeric(df['Age'], errors='coerce')\n",
    "    \n",
    "    # Enforce date format\n",
    "    df['Date_of_Admission'] = pd.to_datetime(\n",
    "        df['Date_of_Admission'], \n",
    "        format='%m/%d/%Y',\n",
    "        errors='coerce'  # Bad dates become NaT (NULL)\n",
    "    )\n",
    "    df['Discharge_Date'] = pd.to_datetime(\n",
    "        df['Discharge_Date'], \n",
    "        format='%m/%d/%Y', \n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # --- 2. Validation: Drop rows with critical missing data ---\n",
    "    # We can use this code segment for any critical columns as needed\n",
    "    initial_rows = len(df)\n",
    "    df.dropna(subset=['Name', 'Date_of_Admission'], inplace=True)\n",
    "    if initial_rows > len(df):\n",
    "        print(f\"Dropped {initial_rows - len(df)} rows due to missing Name or Admission Date.\")\n",
    "\n",
    "    # --- 3. Deduplication (within the batch) ---\n",
    "    # Remove fully duplicated rows\n",
    "    initial_rows = len(df)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    if initial_rows > len(df):\n",
    "        print(f\"Dropped {initial_rows - len(df)} fully duplicate rows.\")\n",
    "\n",
    "    # Keep min age for rows duplicated on all other fields\n",
    "    initial_rows = len(df)\n",
    "    group_cols = [col for col in df.columns if col not in ['Age']]\n",
    "    \n",
    "    if group_cols:\n",
    "        # We must re-aggregate all non-group columns\n",
    "        agg_dict = {col: 'first' for col in df.columns if col not in group_cols}\n",
    "        agg_dict['Age'] = 'min'\n",
    "        \n",
    "        df = df.groupby(group_cols, as_index=False).agg(agg_dict)\n",
    "\n",
    "    if initial_rows > len(df):\n",
    "        print(f\"Consolidated {initial_rows - len(df)} rows based on minimum age.\")\n",
    "\n",
    "    # --- 4. Create the Unique Business Key (SourceAdmissionID) ---\n",
    "    # This key uniquely identifies an admission event for idempotency.\n",
    "    key_cols = ['Name', 'Date_of_Admission', 'Doctor', 'Hospital', 'Medical_Condition']\n",
    "    \n",
    "    # Create a stable composite key string\n",
    "    df['composite_key'] = df[key_cols].fillna('').astype(str).apply(lambda x: '|'.join(x), axis=1)\n",
    "    \n",
    "    # Create a SHA-256 hash (64 chars)\n",
    "    df['SourceAdmissionID'] = df['composite_key'].apply(\n",
    "        lambda x: hashlib.sha256(x.encode()).hexdigest()\n",
    "    )\n",
    "    \n",
    "    # Drop the temporary helper column\n",
    "    df.drop(columns=['composite_key'], inplace=True)\n",
    "\n",
    "    print(f\"Validation complete. {len(df)} clean rows ready for staging.\")\n",
    "    return df # Return the cleaned DataFrame for loading to the staging table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a938c4d-3df3-4826-8531-fc695fa5e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the User Defined Function responsible for loading the cleaned data into the staging table.\n",
    "def load_to_staging(df, db_engine):\n",
    "    \"\"\"\n",
    "    Loads the clean DataFrame into the Staging_Admissions table.\n",
    "    This process is a full TRUNCATE and load of the *batch*.\n",
    "    \"\"\"\n",
    "    print(\"Loading data to staging table...\")\n",
    "    try:\n",
    "        with db_engine.connect() as conn:\n",
    "            # We use a transaction for the staging load\n",
    "            with conn.begin():\n",
    "                conn.execute(text(\"SET FOREIGN_KEY_CHECKS = 0;\")) # Disable FK checks for truncation\n",
    "                conn.execute(text(\"TRUNCATE TABLE Staging_Admissions;\")) # Clear existing staging data\n",
    "                conn.execute(text(\"SET FOREIGN_KEY_CHECKS = 1;\"))# Enable FK checks\n",
    "                \n",
    "                # Load the cleaned dataframe\n",
    "                df.to_sql('Staging_Admissions', con=conn, if_exists='append', index=False)\n",
    "        \n",
    "        print(f\"Successfully loaded {len(df)} rows to Staging_Admissions.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading to staging: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83e6e90e-2285-4e20-8704-b3267045da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start loading the data warehouse from the staging admission table into the fact and dimensions\n",
    "def run_data_warehouse_load(db_engine):\n",
    "    \"\"\"\n",
    "    Executes the incremental stored procedure in MySQL.\n",
    "    \"\"\"\n",
    "    print(\"Calling stored procedure sp_LoadDataWarehouse_Incremental...\")\n",
    "    try:\n",
    "        with db_engine.connect() as conn:\n",
    "            with conn.begin():\n",
    "                # The stored procedure handles its own transaction\n",
    "                conn.execute(text(\"CALL sp_LoadDataWarehouse_Incremental();\"))\n",
    "        \n",
    "        print(\"Data warehouse load procedure executed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running stored procedure: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "560ea2e4-2137-467b-aad1-14805b11272e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting extraction and validation for: /Users/waleedmouhammed/Library/CloudStorage/OneDrive-Personal/Desktop/Healthcare Analytics Project/Raw Data CSV.csv\n",
      "Dropped 534 fully duplicate rows.\n",
      "Consolidated 4966 rows based on minimum age.\n",
      "Validation complete. 50000 clean rows ready for staging.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 16 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   Name                50000 non-null  object        \n",
      " 1   Gender              50000 non-null  object        \n",
      " 2   Blood_Type          50000 non-null  object        \n",
      " 3   Medical_Condition   50000 non-null  object        \n",
      " 4   Date_of_Admission   50000 non-null  datetime64[ns]\n",
      " 5   Doctor              50000 non-null  object        \n",
      " 6   Hospital            50000 non-null  object        \n",
      " 7   Insurance_Provider  50000 non-null  object        \n",
      " 8   Billing_Amount      50000 non-null  float64       \n",
      " 9   Room_Number         50000 non-null  int64         \n",
      " 10  Admission_Type      50000 non-null  object        \n",
      " 11  Discharge_Date      50000 non-null  datetime64[ns]\n",
      " 12  Medication          50000 non-null  object        \n",
      " 13  Test_Results        50000 non-null  object        \n",
      " 14  Age                 50000 non-null  int64         \n",
      " 15  SourceAdmissionID   50000 non-null  object        \n",
      "dtypes: datetime64[ns](2), float64(1), int64(2), object(11)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = '/Users/waleedmouhammed/Library/CloudStorage/OneDrive-Personal/Desktop/Healthcare Analytics Project/Raw Data CSV.csv' \n",
    "clean_df = extract_and_validate_csv(csv_file_path)\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a3a722a-b836-4f9b-8c8c-adfec6f996a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting extraction and validation for: /Users/waleedmouhammed/Library/CloudStorage/OneDrive-Personal/Desktop/Healthcare Analytics Project/Raw Data CSV.csv\n",
      "Dropped 534 fully duplicate rows.\n",
      "Consolidated 4966 rows based on minimum age.\n",
      "Validation complete. 50000 clean rows ready for staging.\n",
      "Loading data to staging table...\n",
      "Successfully loaded 50000 rows to Staging_Admissions.\n",
      "Calling stored procedure sp_LoadDataWarehouse_Incremental...\n",
      "Data warehouse load procedure executed successfully.\n",
      "\n",
      "ETL process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # You would pass this in as an argument in a real script\n",
    "    csv_file_path = '/Users/waleedmouhammed/Library/CloudStorage/OneDrive-Personal/Desktop/Healthcare Analytics Project/Raw Data CSV.csv' \n",
    "    \n",
    "    try:\n",
    "        # 1. Extract, Transform, Validate\n",
    "        clean_df = extract_and_validate_csv(csv_file_path)\n",
    "        \n",
    "        if clean_df is not None and not clean_df.empty:\n",
    "            # 2. Load to Staging\n",
    "            load_to_staging(clean_df, engine)\n",
    "            \n",
    "            # 3. Load to Data Warehouse\n",
    "            run_data_warehouse_load(engine)\n",
    "            \n",
    "            print(\"\\nETL process completed successfully!\")\n",
    "        \n",
    "        else:\n",
    "            print(\"ETL process halted: No valid data to load.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nETL process FAILED: {e}\")\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hca-venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

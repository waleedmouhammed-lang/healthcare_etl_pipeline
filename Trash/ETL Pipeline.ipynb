{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf6e5bd3-9c20-4b88-a55d-cacd0633929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "import pymysql\n",
    "from dotenv import load_dotenv  # For securely managing credentials\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71054c4e-1f44-4d4f-8e1a-82f7dd089e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION: Load credentials from a .env file ---\n",
    "# Create a file named \".env\" in the same directory with:\n",
    "# DB_USER=\"your_user\"\n",
    "# DB_PASS=\"your_password\"\n",
    "# DB_HOST=\"localhost\"\n",
    "# DB_NAME=\"HealthcareADT_DW\"\n",
    "#load_dotenv()\n",
    "\n",
    "DB_USER = 'root'\n",
    "DB_PASS = 'root@1234'\n",
    "DB_HOST = 'localhost'\n",
    "DB_NAME = 'HealthcareADT_DW'\n",
    "\n",
    "if not all([DB_USER, DB_PASS, DB_HOST, DB_NAME]):\n",
    "    print(\"Error: Database credentials are not set in .env file.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b73490de-c4fc-4b9a-8f1f-d47fea1cd921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding password\n",
    "import urllib.parse\n",
    "\n",
    "encoded_password = urllib.parse.quote(DB_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6a57ae4-9029-4f3e-8b01-0fe3bdbd6c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection engine created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a database connection engine\n",
    "try:\n",
    "    connection_string = f\"mysql+pymysql://{DB_USER}:{encoded_password}@{DB_HOST}/{DB_NAME}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    print(\"Database connection engine created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating database engine: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fabb3611-6945-4167-879e-0a9899ab0c6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect(engine).get_table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de1c44a-b396-4616-871b-70129bd19cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the User Defined Function responsible for extracting data from the main csv file.\n",
    "def extract_and_validate_csv(file_path):\n",
    "    \"\"\"\n",
    "    Extracts data from CSV, validates, cleans, transforms, and\n",
    "    creates a unique business key.\n",
    "    \"\"\"\n",
    "    print(f\"Starting extraction and validation for: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV: {e}\")\n",
    "        return None\n",
    "    # #######################\n",
    "    # --- Amending the original dataset column names to fit the script names\n",
    "    df.columns = [col.replace(\" \", \"_\") for col in df.columns]\n",
    "    # #######################\n",
    "    # --- 1. Data Cleaning ---\n",
    "    # Trim all string columns\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = df[col].str.strip()\n",
    "\n",
    "    # Convert numeric and date columns, handling errors\n",
    "    df['Billing_Amount'] = pd.to_numeric(\n",
    "        df['Billing_Amount'].astype(str).str.replace(r'[$,]', '', regex=True),\n",
    "        errors='coerce'  # Bad values become NaN (NULL)\n",
    "    )\n",
    "    df['Age'] = pd.to_numeric(df['Age'], errors='coerce')\n",
    "    \n",
    "    # Enforce date format\n",
    "    df['Date_of_Admission'] = pd.to_datetime(\n",
    "        df['Date_of_Admission'], \n",
    "        format='%m/%d/%Y',\n",
    "        errors='coerce'  # Bad dates become NaT (NULL)\n",
    "    )\n",
    "    df['Discharge_Date'] = pd.to_datetime(\n",
    "        df['Discharge_Date'], \n",
    "        format='%m/%d/%Y', \n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # --- 2. Validation: Drop rows with critical missing data ---\n",
    "    initial_rows = len(df)\n",
    "    df.dropna(subset=['Name', 'Date_of_Admission'], inplace=True)\n",
    "    if initial_rows > len(df):\n",
    "        print(f\"Dropped {initial_rows - len(df)} rows due to missing Name or Admission Date.\")\n",
    "\n",
    "    # --- 3. Deduplication (within the batch) ---\n",
    "    # Remove fully duplicated rows\n",
    "    initial_rows = len(df)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    if initial_rows > len(df):\n",
    "        print(f\"Dropped {initial_rows - len(df)} fully duplicate rows.\")\n",
    "\n",
    "    # Keep min age for rows duplicated on all other fields\n",
    "    initial_rows = len(df)\n",
    "    group_cols = [col for col in df.columns if col not in ['Age']]\n",
    "    \n",
    "    if group_cols:\n",
    "        # We must re-aggregate all non-group columns\n",
    "        agg_dict = {col: 'first' for col in df.columns if col not in group_cols}\n",
    "        agg_dict['Age'] = 'min'\n",
    "        \n",
    "        df = df.groupby(group_cols, as_index=False).agg(agg_dict)\n",
    "\n",
    "    if initial_rows > len(df):\n",
    "        print(f\"Consolidated {initial_rows - len(df)} rows based on minimum age.\")\n",
    "\n",
    "    # --- 4. Create the Unique Business Key (SourceAdmissionID) ---\n",
    "    # This key uniquely identifies an admission event for idempotency.\n",
    "    key_cols = ['Name', 'Date_of_Admission', 'Doctor', 'Hospital', 'Medical_Condition']\n",
    "    \n",
    "    # Create a stable composite key string\n",
    "    df['composite_key'] = df[key_cols].fillna('').astype(str).apply(lambda x: '|'.join(x), axis=1)\n",
    "    \n",
    "    # Create a SHA-256 hash (64 chars)\n",
    "    df['SourceAdmissionID'] = df['composite_key'].apply(\n",
    "        lambda x: hashlib.sha256(x.encode()).hexdigest()\n",
    "    )\n",
    "    \n",
    "    # Drop the temporary helper column\n",
    "    df.drop(columns=['composite_key'], inplace=True)\n",
    "\n",
    "    print(f\"Validation complete. {len(df)} clean rows ready for staging.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a938c4d-3df3-4826-8531-fc695fa5e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the User Defined Function responsible for loading the cleaned data into the staging\n",
    "# table in the database\n",
    "def load_to_staging(df, db_engine):\n",
    "    \"\"\"\n",
    "    Loads the clean DataFrame into the Staging_Admissions table.\n",
    "    This process is a full TRUNCATE and load of the *batch*.\n",
    "    \"\"\"\n",
    "    print(\"Loading data to staging table...\")\n",
    "    try:\n",
    "        with db_engine.connect() as conn:\n",
    "            # We use a transaction for the staging load\n",
    "            with conn.begin():\n",
    "                conn.execute(text(\"SET FOREIGN_KEY_CHECKS = 0;\"))\n",
    "                conn.execute(text(\"TRUNCATE TABLE Staging_Admissions;\"))\n",
    "                conn.execute(text(\"SET FOREIGN_KEY_CHECKS = 1;\"))\n",
    "                \n",
    "                # Load the *cleaned* dataframe\n",
    "                df.to_sql('Staging_Admissions', con=conn, if_exists='append', index=False)\n",
    "        \n",
    "        print(f\"Successfully loaded {len(df)} rows to Staging_Admissions.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading to staging: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83e6e90e-2285-4e20-8704-b3267045da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start loading the data warehouse from the staging admission table into the fact and dimensions\n",
    "def run_data_warehouse_load(db_engine):\n",
    "    \"\"\"\n",
    "    Executes the incremental stored procedure in MySQL.\n",
    "    \"\"\"\n",
    "    print(\"Calling stored procedure sp_LoadDataWarehouse_Incremental...\")\n",
    "    try:\n",
    "        with db_engine.connect() as conn:\n",
    "            with conn.begin():\n",
    "                # The stored procedure handles its own transaction\n",
    "                conn.execute(text(\"CALL sp_LoadDataWarehouse_Incremental();\"))\n",
    "        \n",
    "        print(\"Data warehouse load procedure executed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running stored procedure: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a3a722a-b836-4f9b-8c8c-adfec6f996a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting extraction and validation for: /Users/waleedmouhammed/Library/CloudStorage/OneDrive-Personal/Desktop/تكامل/Health Care Analytics/Raw Data/Raw Data CSV.csv\n",
      "Dropped 534 fully duplicate rows.\n",
      "Consolidated 4966 rows based on minimum age.\n",
      "Validation complete. 50000 clean rows ready for staging.\n",
      "Loading data to staging table...\n",
      "Error loading to staging: (pymysql.err.ProgrammingError) (1146, \"Table 'healthcareadt_dw.staging_admissions' doesn't exist\")\n",
      "[SQL: TRUNCATE TABLE Staging_Admissions;]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "\n",
      "ETL process FAILED: (pymysql.err.ProgrammingError) (1146, \"Table 'healthcareadt_dw.staging_admissions' doesn't exist\")\n",
      "[SQL: TRUNCATE TABLE Staging_Admissions;]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/waleedmouhammed/Library/CloudStorage/OneDrive-Personal/Desktop/Healthcare Analytics Project/hca-venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # You would pass this in as an argument in a real script\n",
    "    csv_file_path = '/Users/waleedmouhammed/Library/CloudStorage/OneDrive-Personal/Desktop/تكامل/Health Care Analytics/Raw Data/Raw Data CSV.csv' \n",
    "    \n",
    "    try:\n",
    "        # 1. Extract, Transform, Validate\n",
    "        clean_df = extract_and_validate_csv(csv_file_path)\n",
    "        \n",
    "        if clean_df is not None and not clean_df.empty:\n",
    "            # 2. Load to Staging\n",
    "            load_to_staging(clean_df, engine)\n",
    "            \n",
    "            # 3. Load to Data Warehouse\n",
    "            run_data_warehouse_load(engine)\n",
    "            \n",
    "            print(\"\\nETL process completed successfully!\")\n",
    "        \n",
    "        else:\n",
    "            print(\"ETL process halted: No valid data to load.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nETL process FAILED: {e}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d05b9c-8cbe-4a1e-a151-568389debcfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hca-venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
